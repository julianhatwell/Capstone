---
title       : Next word prediction app
subtitle    : The Data Science Specialisation Capstone Project
author      : Julian Hatwell
job         : Analyst
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : standalone    # {draft, selfcontained}
knit        : slidify::knit2slides
--- 

## Introduction & Motivation

This presentation is the last part of the Coursera Data Science Specialisation Capstone Project deliverables. 

The objective of the project, which has been conducted in partnership with Swiftkey and inspired by their app, is to create an algorithm that can predict the next word given a text input and deliver this into production as a hosted ShinyApp.

<div style='text-align: center;'>
    <img height='250' src='Capture.png' />
</div>


A screenshot from the finished ShinyApp showing the key features.

---
 
## Theory & Practice

The theories behind [text mining](https://en.wikipedia.org/wiki/Text_mining) first appeared in the 1980's. A typical approach is to create an nGram model and determine the distribution and probability of various words and sentences from this model. An nGram is a contiguous sequence of words of length n that appear somewhere in the source document.

These theories are now sufficiently mature for there to be widely available tooling for various platforms. The ["tm" package](https://cran.r-project.org/web/packages/tm/index.html) for R has been used extensively to carry out this project.

[Functions](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) in the ["tm" package](https://cran.r-project.org/web/packages/tm/index.html) make it trivial to determine e.g. what are the most frequently used terms, as well as more complicated lookups e.g. where nGrams appear to be associated by their appearance together in multiple source documents.

SwiftKey have kindly provided three sources of user generated content based on Blogs, News and Twitter. These text documents are sampled, tokenised (to identify individual words within text) and converted into nGrams.

---

## Word Prediction App for the Capstone Project

The app can be found [here](https://julianhatwell.shinyapps.io/wordpredict). It is very easy to use:

* Select the type of prediction from "General," "Formal" or "Tweet."
    + App will dynamically switch to between all, blog & news or twitter search space
    + Useful if a phone app can detect what the user is working on and tailor prediction
* Start typing text:
    + After some letters but before any spaces, predictions are for first word completion
    + After the first space following some word/characters, the model changes to predict the next word
* Special Features:
    + Top One Prediction is unambiguously displayed
    + Buttons dynamically update with top 3 and can be used to update the input
    + Graph shows displays certainty of prediction as probabilities

---

## Appendix

Project Files: [Interim Milestone Report](http://rpubs.com/julianhatwell/ds_cap_ms_rep), [Gihub Repo](https://github.com/julianhatwell/ds_capstone) and [The Word Predict App](https://julianhatwell.shinyapps.io/wordpredict/)

Algorithms researched and tested (the last one chosen for the app):
* A naive backoff (without smoothing)
* Interplotation
* Good-Turing Smoothing
* Katz backoff, using Good-Turing Smoothing

I had the idea to include lazy buttons to give the app a dynamic element. I used [this code example](https://github.com/bergiste/coursera-final-capstone/blob/master/shiny-app/server.R) as a starting point and adapted it for my own needs. All modeling, smoothing and plotting code is otherwise my own work.

References: [Anoop Sarkhar's Notes](http://anoopsarkar.github.io/nlp-class/assets/slides/lm.pdf), [N Chambers Lecture Notes](http://www.usna.edu/Users/cs/nchamber/courses/nlp/f12/slides/set4-smoothing.pdf), [Cornell University's lecture notes ](http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures.htm), [Presentation from Stanford University](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)