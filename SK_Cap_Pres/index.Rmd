---
title       : Next word prediction app
subtitle    : The Data Science Specialisation Capstone Project
author      : Julian Hatwell
job         : Analyst
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : standalone    # {draft, selfcontained}
knit        : slidify::knit2slides
--- 

## Introduction & Motivation

This presentation is the last part of the Coursera Data Science Specialisation Capstone Project deliverables. 

The objective of the project, which has been conducted in partnership with Swiftkey and inspired by their app, is to create an algorithm that can predict the next word given a text input and deliver this into production as a hosted ShinyApp.

<div style='text-align: center;'>
    <img height='250' src='https://raw.githubusercontent.com/julianhatwell/ds_capstone/master/SK_Cap_Pres/Capture.PNG' />
</div>

A screenshot from the finished ShinyApp showing the key features.

---

## App Features

The app can be found [here](https://julianhatwell.shinyapps.io/wordpredict). It is very easy to use:

* Select the type of prediction from "General," "Formal" or "Tweet."
    + App will dynamically switch to between all, blog & news or twitter search space
    + Useful if a phone app can detect what the user is working on and tailor prediction
* Start typing text:
    + After some letters but before any spaces, predictions are for first word completion
    + After the first space following some word/characters, the model changes to predict the next word
* Special Features:
    + Top One Prediction is unambiguously displayed
    + Buttons dynamically update with top 3 and can be used to update the input
    + Graph displays certainty of prediction as probabilities of each word

---

## Prediction Algorithm

nGrams have their counts adjusted using Good-Turing smoothing to reduce over-fitting. Probabilities of very rare nGrams are reduced. c is nGram count and n is the number of nGrams with count c:
$$c^*=(c+1)\frac{n_{c+1}}{n_c}$$

Very low (< $5e^{-6}$) Maximum Likelhood nGrams are unlikely to be predicted, so are removed from the model. ML simply calculated as:
$$P_{ML}(nGram) = P(w_i)P(w_{i-1})P(w_{i-2})...P(w_{i-n})$$

Katz backoff finds conditional probabilities for candidate predictions using the adjusted count and backs off to a lower order nGram if no match found. $\alpha$ tunes down the lower order nGram probabilities. The candidate with the highest probability is chosen:
$$\begin{equation}
  P_{katz}(y|x)=\begin{cases}
    \frac{c^*(x, y)}{c(x)}, & \text{if $c(xy) > 0$}.\\
    \alpha(x)P_{katz}(y), & \text{otherwise}.
  \end{cases}
\end{equation}$$

---

## Appendix

Project Files: [Interim Milestone Report](http://rpubs.com/julianhatwell/ds_cap_ms_rep), [Gihub Repo](https://github.com/julianhatwell/ds_capstone) and [The Word Predict App](https://julianhatwell.shinyapps.io/wordpredict/)

Algorithms researched and tested (the last one chosen for the app):
* A naive backoff (without smoothing)
* Interplotation
* Good-Turing Smoothing
* Katz backoff, using Good-Turing Smoothing

I had the idea to include lazy buttons to give the app a dynamic element. I used [this code example](https://github.com/bergiste/coursera-final-capstone/blob/master/shiny-app/server.R) as a starting point and adapted it for my own needs. All modeling, smoothing and plotting code is otherwise my own work.

References: [Anoop Sarkhar's Notes](http://anoopsarkar.github.io/nlp-class/assets/slides/lm.pdf), [N Chambers Lecture Notes](http://www.usna.edu/Users/cs/nchamber/courses/nlp/f12/slides/set4-smoothing.pdf), [Cornell University's lecture notes ](http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures.htm), [Presentation from Stanford University](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)