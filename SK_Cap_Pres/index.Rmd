---
title       : Next word prediction app
subtitle    : The Data Science Specialisation Capstone Project
author      : Julian Hatwell
job         : Analyst
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : standalone    # {draft, selfcontained}
knit        : slidify::knit2slides
--- 

## Introduction & Motivation

The Coursera Data Science Specialisation is by now a widely recognised MOOC, conducted over 9 modules plus a Capstone Project. Taking 10-12 months to complete.

This presentation is part of the final output of the Capstone Project, which has been conducted in partnership with Swiftkey.

The objective (inspired by mobile apps such as SwiftKey) is to create an algorithm that can predict the next word given a text input and deliver this into production as a hosted ShinyApp.

---
 
## Theory & Practice

The theories behind [text mining](https://en.wikipedia.org/wiki/Text_mining) first appeared in the 1980's. A typical approach is to create an nGram model and determine the distribution and probability of various words and sentences from this model. An nGram is a contiguous sequence of words of length n that appear somewhere in the source document.

These theories are now sufficiently mature for there to be widely available tooling for various platforms. The ["tm" package](https://cran.r-project.org/web/packages/tm/index.html) for R has been used extensively to carry out this project.

[Functions](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) in the ["tm" package](https://cran.r-project.org/web/packages/tm/index.html) make it trivial to determine e.g. what are the most frequently used terms, as well as more complicated lookups e.g. where nGrams appear to be associated by their appearance together in multiple source documents.

SwiftKey have kindly provided three sources of user generated content based on Blogs, News and Twitter. These text documents are sampled, tokenised (to identify individual words within text) and converted into nGrams.

---

## Word Prediction App for the Capstone Project

The app can be found [here](https://julianhatwell.shinyapps.io/wordpredict). 

It is very easy to use and implements the following features:

* Select the type of prediction from "General," "Formal" or "Tweet."
    + App will dynamically switch to appropriate search space to povide a tailored prediction
    + In the SwiftKey setting, it could detect the application the user is working on such as blogging or Twitter
* Start typing text:
    + After some letters are typed but before any spaces, partial matches return a prediction for first word completion
    + After the first space following some word/characters, algorithm moves to the nGram model to predict the next word

---

## Appendix

[Interim Milestone Report](http://rpubs.com/julianhatwell/ds_cap_ms_rep), [Gihub Repo](https://github.com/julianhatwell/SK_Cap) and [The Word Predict App](https://julianhatwell.shinyapps.io/wordpredict/)

Algorithms researched and tested:
* A naive backoff (without smoothing)
* Interplotation
* Good-Turing Smoothing

References: [Cornell University's lecture notes ](http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures.htm), [Presentation from Stanford University](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf), [Anoop Sarkhar's Notes](http://anoopsarkar.github.io/nlp-class/assets/slides/lm.pdf), [N Chambers Lecture Notes](http://www.usna.edu/Users/cs/nchamber/courses/nlp/f12/slides/set4-smoothing.pdf), [Code example for active input buttons](https://github.com/bergiste/coursera-final-capstone/blob/master/shiny-app/server.R)

---